{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3092ORVDH8Tx",
        "outputId": "d0eecc49-506f-411a-b373-39f7b8782be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/My Drive/Applied CV Project/freiburg_groceries_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPJiff4qIM2T",
        "outputId": "438a83d2-7fa3-4334-adb7-28e2c07662eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1i0IgUBpr8uyXxhLQgE5RaQK7Qf6agpOh/Applied CV Project/freiburg_groceries_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPDr7_rBSD_P",
        "outputId": "7da68a7f-f56a-499e-a06c-5f2ef1e1afe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mBEANS\u001b[0m/  \u001b[01;34mCEREAL\u001b[0m/     \u001b[01;34mCOFFEE\u001b[0m/  \u001b[01;34mFLOUR\u001b[0m/  \u001b[01;34mJUICE\u001b[0m/  \u001b[01;34mOIL\u001b[0m/    \u001b[01;34mSODA\u001b[0m/    \u001b[01;34mTEA\u001b[0m/            train_data.json\n",
            "\u001b[01;34mCAKE\u001b[0m/   \u001b[01;34mCHIPS\u001b[0m/      \u001b[01;34mCORN\u001b[0m/    \u001b[01;34mHONEY\u001b[0m/  \u001b[01;34mMILK\u001b[0m/   \u001b[01;34mPASTA\u001b[0m/  \u001b[01;34mSPICES\u001b[0m/  test_data.json  \u001b[01;34mVINEGAR\u001b[0m/\n",
            "\u001b[01;34mCANDY\u001b[0m/  \u001b[01;34mCHOCOLATE\u001b[0m/  \u001b[01;34mFISH\u001b[0m/    \u001b[01;34mJAM\u001b[0m/    \u001b[01;34mNUTS\u001b[0m/   \u001b[01;34mRICE\u001b[0m/   \u001b[01;34mSUGAR\u001b[0m/   \u001b[01;34mTOMATO_SAUCE\u001b[0m/   \u001b[01;34mWATER\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "N4Q2kXLONGmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf20726-51d1-41a5-a73a-59a1c92f39c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-dhr1i7om\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-dhr1i7om\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import clip\n",
        "from transformers import CLIPProcessor, CLIPModel"
      ],
      "metadata": {
        "id": "GzUXV27SIxZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose computation device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load pre-trained CLIP model\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ],
      "metadata": {
        "id": "7pESWhATMAxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class image_title_dataset(Dataset):\n",
        "    def __init__(self, list_image_path, list_txt):\n",
        "        self.image_path = list_image_path\n",
        "        self.title = clip.tokenize(list_txt)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            image = preprocess(Image.open(self.image_path[idx]))\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {self.image_path[idx]}. Skipping...\")\n",
        "            return None, None\n",
        "\n",
        "        title = self.title[idx]\n",
        "        return image, title"
      ],
      "metadata": {
        "id": "ciFvpexEik2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(json_path):\n",
        "  with open(json_path, \"r\") as file:\n",
        "      json_data = json.load(file)\n",
        "  input_data = []\n",
        "  for item in json_data:\n",
        "    input_data.append(item)\n",
        "  list_image_path = []\n",
        "  list_txt = []\n",
        "  for item in input_data:\n",
        "    # img_path = image_path + item['image_path'].split('/')[-1]\n",
        "    img_path = item['file_path']\n",
        "    # caption = item['product_title'][:40]\n",
        "    caption = item['labels'][0]\n",
        "    list_image_path.append(img_path)\n",
        "    list_txt.append(caption)\n",
        "  return list_image_path, list_txt"
      ],
      "metadata": {
        "id": "u6NwNS5ohvWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = './train_data.json'\n",
        "test_path = './test_data.json'\n",
        "\n",
        "train_data = load_data(train_path)\n",
        "test_data = load_data(test_path)\n",
        "train_set = image_title_dataset(train_data[0], train_data[1])\n",
        "test_set = image_title_dataset(test_data[0], test_data[1])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "# Print the size of train loader\n",
        "print(f\"Size of train_loader: {len(train_loader)}\")\n",
        "\n",
        "# Print the size of test loader\n",
        "print(f\"Size of test_loader: {len(test_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghU8qLeb6ik_",
        "outputId": "a57834f1-b29e-43df-97b3-55260888861a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train_loader: 63\n",
            "Size of test_loader: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert model's parameters to FP32 format\n",
        "def convert_models_to_fp32(model):\n",
        "    for p in model.parameters():\n",
        "        p.data = p.data.float()\n",
        "        p.grad.data = p.grad.data.float()\n",
        "\n",
        "\n",
        "if device == \"cpu\":\n",
        "  model.float()\n",
        "\n",
        "# Prepare the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
        "\n",
        "# Specify the loss function\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "pLdW8NvDIx0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables for early stopping\n",
        "best_accuracy = 0\n",
        "patience = 1  # Number of epochs to wait for improvement\n",
        "wait_count = 0\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 3  # Adjust as needed\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, total=len(train_loader))\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, texts = batch\n",
        "\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "        # Compute loss\n",
        "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
        "        total_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted_img = torch.max(logits_per_image, 1)\n",
        "        _, predicted_text = torch.max(logits_per_text, 1)\n",
        "        total_correct += (predicted_img == ground_truth).sum().item() + (predicted_text == ground_truth).sum().item()\n",
        "        total_samples += ground_truth.size(0)\n",
        "\n",
        "        # Backward pass\n",
        "        total_loss.backward()\n",
        "        if device != \"cpu\":\n",
        "            convert_models_to_fp32(model)\n",
        "            optimizer.step()\n",
        "            clip.model.convert_weights(model)\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "    # Print accuracy at the end of each epoch\n",
        "    accuracy = total_correct / total_samples\n",
        "    print(f\"Accuracy at epoch {epoch+1}: {accuracy:.4f}\")\n",
        "\n",
        "    # Check for improvement in accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        wait_count = 0\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), \"clip_freiburg.pth\")\n",
        "        print(\"Model saved.\")\n",
        "    else:\n",
        "        wait_count += 1\n",
        "        if wait_count >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3l8u0n9IK_t",
        "outputId": "e4733f17-4e25-460a-a055-0bba99220b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3, Loss: 1.8066: 100%|██████████| 63/63 [00:36<00:00,  1.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy at epoch 1: 0.5322\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3, Loss: 1.8525: 100%|██████████| 63/63 [00:37<00:00,  1.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy at epoch 2: 0.6035\n",
            "Model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3, Loss: 2.1914: 100%|██████████| 63/63 [00:37<00:00,  1.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy at epoch 3: 0.5655\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "\n",
        "# Initialize tqdm progress bar\n",
        "pbar = tqdm(test_loader, total=len(test_loader), desc=\"Testing\")\n",
        "\n",
        "# Iterate over the test data\n",
        "for batch in pbar:\n",
        "    images, texts = batch\n",
        "    images = images.to(device)\n",
        "    texts = texts.to(device)\n",
        "\n",
        "    with torch.no_grad():  # No need to track gradients during inference\n",
        "        # Forward pass\n",
        "        logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "        # Compute accuracy\n",
        "        _, predicted_img = torch.max(logits_per_image, 1)\n",
        "        _, predicted_text = torch.max(logits_per_text, 1)\n",
        "        test_correct += (predicted_img == predicted_text).sum().item()  # Assuming images and texts are matched\n",
        "        test_total += images.size(0)\n",
        "\n",
        "    # Update tqdm progress bar description\n",
        "    pbar.set_postfix({'Test Accuracy': test_correct / test_total})\n",
        "\n",
        "# Calculate accuracy\n",
        "test_accuracy = test_correct / test_total\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VP76J7dAeDu",
        "outputId": "12512964-6360-4afa-8b9a-6cb069540d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 16/16 [02:57<00:00, 11.11s/it, Test Accuracy=0.00294]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}